# Project: Data Modeling with Postgres
## Introduction
A startup called Sparkify wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. The analytics team is particularly interested in understanding what songs users are listening to. Currently, they don't have an easy way to query their data, which resides in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

## Project Description

### This project includes the following stages
In this project, you'll apply what you've learned on data modeling with Postgres and build an ETL pipeline using Python. To complete the project, you will need to define fact and dimension tables for a star schema for a particular analytic focus, and write an ETL pipeline that transfers data from files in two local directories into these tables in Postgres using Python and SQL.
1. Data modeling with Postgres
2. Database star schema created 
3. ETL pipeline using Python

## Datasets
### Song Dataset
The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.

### Log Dataset
The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.

## Database Schema
The schema used for this exercise is the Star Schema: 
There is one main fact table containing all the measures associated to each event (user song plays), 
and 4 dimentional tables, each with a primary key that is being referenced from the fact table.

On why to use a relational database for this case:
- The data types are structured (we know before-hand the sctructure of the jsons we need to analyze, and where and how to extract and transform each field)
- The amount of data we need to analyze is not big enough to require big data related solutions.
- Ability to use SQL that is more than enough for this kind of analysis
- Data needed to answer business questions can be modeled using simple ERD models
- We need to use JOINS for this scenario

### Song Plays table

- *Name:* `songplays`
- *Type:* Fact table

| Column | Type | Description |
| ------ | ---- | ----------- |
| `songplay_id` | `INTEGER` | The main identification of the table | 
| `start_time` | `TIMESTAMP NOT NULL` | The timestamp that this song play log happened |
| `user_id` | `INTEGER NOT NULL REFERENCES users (user_id)` | The user id that triggered this song play log. It cannot be null, as we don't have song play logs without being triggered by an user.  |
| `level` | `VARCHAR NULL` | The level of the user that triggered this song play log |
| `song_id` | `VARCHAR REFERENCES songs (song_id)` | The identification of the song that was played. It can be null.  |
| `artist_id` | `VARCHAR REFERENCES artists (artist_id)` | The identification of the artist of the song that was played. |
| `session_id` | `INTEGER NOT NULL` | The session_id of the user on the app |
| `location` | `VARCHAR NULL` | The location where this song play log was triggered  |
| `user_agent` | `VARCHAR NULL` | The user agent of our app |

### Users table

- *Name:* `users`
- *Type:* Dimension table

| Column | Type | Description |
| ------ | ---- | ----------- |
| `user_id` | `INTEGER PRIMARY KEY` | The main identification of an user |
| `first_name` | `VARCHAR NOT NULL` | First name of the user, can not be null. It is the basic information we have from the user |
| `last_name` | `VARCHAR NOT NULL` | Last name of the user. |
| `gender` | `VARCHAR NULL` | The gender is stated with just one character `M` (male) or `F` (female). Otherwise it can be stated as `NULL` |
| `level` | `VARCHAR NOT NULL` | The level stands for the user app plans (`premium` or `free`) |


### Songs table

- *Name:* `songs`
- *Type:* Dimension table

| Column | Type | Description |
| ------ | ---- | ----------- |
| `song_id` | `VARCHAR PRIMARY KEY` | The main identification of a song | 
| `title` | `VARCHAR NOT NULL` | The title of the song. It can not be null, as it is the basic information we have about a song. |
| `artist_id` | `VARCHAR NOT NULL` | The artist id, it can not be null as we don't have songs without an artist. |
| `year` | `INTEGER NOT NULL` | The year that this song was made |
| `duration` | `FLOAT NOT NULL` | The duration of the song |


### Artists table

- *Name:* `artists`
- *Type:* Dimension table

| Column | Type | Description |
| ------ | ---- | ----------- |
| `artist_id` | `VARCHAR PRIMARY KEY` | The main identification of an artist |
| `name` | `VARCHAR NOT NULL` | The name of the artist |
| `location` | `VARCHAR NOT NULL` | The location where the artist are from |
| `latitude` | `FLOAT NULL` | The latitude of the location that the artist are from |
| `longitude` | `FLOAT NULL` | The longitude of the location that the artist are from |

### Time table

- *Name:* `time`
- *Type:* Dimension table

| Column | Type | Description |
| ------ | ---- | ----------- |
| `start_time` | `TIMESTAMP NOT NULL PRIMARY KEY` | The timestamp itself, serves as the main identification of this table |
| `hour` | `INTEGER NOT NULL` | The hour from the timestamp  |
| `day` | `INTEGER NOT NULL` | The day of the month from the timestamp |
| `week` | `INTEGER NOT NULL` | The week of the year from the timestamp |
| `month` | `INTEGER NOT NULL` | The month of the year from the timestamp |
| `year` | `INTEGER NOT NULL` | The year from the timestamp |
| `weekday` | `INTEGER NOT NULL` | The week day from the timestamp |

## The project file structure

We have a small list of files, easy to maintain and understand:
 - `sql_queries.py` - Where it all begins, this files is meant to be a query repository to use throughout the ETL process
 - `create_tables.py` - It's the file reponsible to create the schema structure into the PostgreSQL database
 - `etl.py` - It's the file responsible for the main ETL process
 - `etl.ipynb` - The python notebook that was written to develop the logic behind the `etl.py` process
 - `test.ipynb` - And finally this notebook was used to certify if our ETL process was being successful (or not).

## How to run the Python scripts
1. Run in console
 ```
python create_tables.py
```
This script drops and creates your tables. You run this file to reset your tables before each time you run your ETL scripts.
2. Run in console
 ```
python etl.py
```
This script reads and processes files from song_data and log_data and loads them into your tables
3. Run test.ipynb which displays the first few rows of each table to let you check your database

