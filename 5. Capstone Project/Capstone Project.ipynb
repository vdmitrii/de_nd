{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "pd.options.display.max_columns = 31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "    This project will pull data from all sources and create fact and dimension tables to show movement of immigration in US by countries by month."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Describe and Gather Data \n",
    "\n",
    "`U.S. City Demographic Data`: comes from OpenSoft and includes data by city, state, age, population, veteran status and race.\n",
    "\n",
    "`I94 Immigration Data` : comes from the US National Tourism and Trade Office and includes details on incoming immigrants and their ports of entry.\n",
    "\n",
    "`Countries`: comes from I94_SAS_Labels_Descriptions.SAS\n",
    "\n",
    "`States`: comes from I94_SAS_Labels_Descriptions.SAS\n",
    "\n",
    "`Visas`: comes from I94_SAS_Labels_Descriptions.SAS\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "input_data = '../../data/18-83510-I94-Data-2016/'\n",
    "immigration_data = input_data + 'i94_apr16_sub.sas7bdat'\n",
    "df_spark = spark.read.format('com.github.saurfang.sas.spark').load(immigration_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#write to parquet\n",
    "# df_spark.write.parquet(\"sas_data\")\n",
    "df_spark = spark.read.parquet(\"sas_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# count rows from df_spark\n",
    "# df_spark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read data about countries from cityres.csv\n",
    "countries_table = spark.read.csv('cityres.csv', header='true')\n",
    "countries_table = countries_table.withColumn(\"code\", F.col(\"code\").cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read data about states with code\n",
    "states_table = pd.read_csv('states_addr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read us-cities-demographics.csv about us demographik situation\n",
    "demographics_table = pd.read_csv('us-cities-demographics.csv', delimiter=';')\n",
    "demographics_table.columns = [col.strip().lower().replace(' ', '_') for col in demographics_table.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "#### Cleaning Steps\n",
    "    Steps for clearing data are indicated in the cells with the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Performing cleaning tasks here\n",
    "# the first thing need to do is rename columns and droping dublikates\n",
    "columns_to_change = ['cicid as cid', 'i94yr as year', 'i94mon as month', 'i94cit as citizenship', 'i94res as residence', 'i94port as airport', \\\n",
    "                     'arrdate as arrival_date', 'i94mode as arrival_mode', 'i94addr as addr_in_us', 'depdate as departure_date', 'i94bir as age', \\\n",
    "                     'i94visa as visa_code', 'count', 'dtadfile as adfile_date', 'visapost', 'occup', \\\n",
    "                     'entdepa as arrival_code', 'entdepd as departure_code', 'entdepu as update_code', 'matflag', 'biryear', \\\n",
    "                     'dtaddto as admitted_date', 'gender', 'insnum', 'airline', 'admnum', 'fltno as flight_number', \\\n",
    "                     'visatype']\n",
    "df_spark = df_spark.selectExpr(columns_to_change).dropDuplicates()\n",
    "\n",
    "# count rows after dropping dublikates\n",
    "# df_spark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# let's change type of columns and Put correct formats in dates \n",
    "df_spark = df_spark\\\n",
    ".withColumn(\"data_base_sas\", F.to_date(F.lit(\"01/01/1960\"), \"MM/dd/yyyy\"))\\\n",
    ".withColumn(\"cid\", F.col(\"cid\").cast(\"integer\"))\\\n",
    ".withColumn(\"year\", F.col(\"year\").cast(\"integer\"))\\\n",
    ".withColumn(\"month\", F.col(\"month\").cast(\"integer\"))\\\n",
    ".withColumn(\"citizenship\", F.col(\"citizenship\").cast(\"integer\"))\\\n",
    ".withColumn(\"residence\", F.col(\"residence\").cast(\"integer\"))\\\n",
    ".withColumn(\"airport\", F.col(\"airport\").cast(\"string\"))\\\n",
    ".withColumn(\"arrival_mode\", F.col(\"arrival_mode\").cast(\"integer\"))\\\n",
    ".withColumn(\"departure_date\", F.expr(\"date_add(data_base_sas, departure_date)\"))\\\n",
    ".withColumn(\"age\", F.col(\"age\").cast(\"integer\"))\\\n",
    ".withColumn(\"visa_code\", F.col(\"visa_code\").cast(\"integer\"))\\\n",
    ".withColumn(\"count\", F.col(\"count\").cast(\"integer\"))\\\n",
    ".withColumn(\"adfile_date\", F.to_date(F.unix_timestamp(F.col(\"adfile_date\"), \"yyyyMMdd\").cast(\"timestamp\")))\\\n",
    ".withColumn(\"biryear\", F.col(\"biryear\").cast(\"integer\"))\\\n",
    ".withColumn(\"admitted_date\", F.to_date(F.unix_timestamp(F.col(\"admitted_date\"), \"MMddyyyy\").cast(\"timestamp\")))\\\n",
    ".withColumn(\"arrival_date\", F.expr(\"date_add(data_base_sas, arrival_date)\"))\\\n",
    ".drop('data_base_sas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# missing values in demographic dataframe replace with mode\n",
    "for column in demographics_table.columns:\n",
    "    demographics_table[column].fillna(demographics_table[column].mode()[0], inplace=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "    I wanted to know more about immigration events, so I put the immigration data at the center of my star schema in a table called `immigration`. The main task is to quickly aggregate and get the necessary information on immigrants by month and by country of origin and country of residence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 3.2 Mapping Out Data Pipelines\n",
    "Let's do steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Start with personal dimensional table\n",
    "personal_columns = ['personal_id','cid', 'age', 'adfile_date', 'occup', 'biryear', 'gender', 'insnum', 'admnum', 'citizenship', 'residence', 'addr_in_us', 'visatype', \\\n",
    "                    'flight_number', 'airline', 'visa_code', 'adfile_date']\n",
    "personal_table = df_spark.withColumn(\"personal_id\", F.monotonically_increasing_id()).select(personal_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "personal_table.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# demographics dimensional table. I don't change something\n",
    "demographics_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# dimensional countries table\n",
    "countries_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# dimensional states table\n",
    "states_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# dim_airport\n",
    "airports_table = df_spark.withColumn(\"airport_id\", F.monotonically_increasing_id()).select('airport_id', 'airport', 'visapost')\n",
    "# airports_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# time table dimension\n",
    "time_table = df_spark.withColumn(\"dayofmonth\", F.dayofmonth(F.col(\"arrival_date\"))) \\\n",
    "                     .withColumn(\"dayofyear\", F.dayofyear(F.col(\"arrival_date\"))) \\\n",
    "                     .withColumn(\"dayofweek\", F.dayofweek(F.col(\"arrival_date\"))) \\\n",
    "                     .select('arrival_date', 'year', 'month', 'dayofmonth', 'dayofyear', 'dayofweek')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "time_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "countries_table.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create fact table immigration\n",
    "immigration_table = df_spark.withColumn(\"id\", F.monotonically_increasing_id()).join(countries_table, df_spark.citizenship == countries_table.code, how='left') \\\n",
    "                            .select('id', 'month', 'country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigration_table.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create ETL for the data model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%file etl.py\n",
    "\n",
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "def create_spark_session():      \n",
    "    spark = SparkSession.builder.\\\n",
    "        config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\n",
    "        enableHiveSupport().\n",
    "        getOrCreate()\n",
    "\n",
    "    return spark\n",
    "\n",
    "def process_countries_data(spark, input_data, output_data):\n",
    "    # read countries data about from cityres.csv\n",
    "    countries_table = spark.read.csv('cityres.csv', header='true')\n",
    "    countries_table = countries_table.withColumn(\"code\", F.col(\"code\").cast(\"integer\"))\n",
    "    \n",
    "    # write to parquet files\n",
    "    countries_table.write.parquet(output_data + 'countries/')\n",
    "    \n",
    "def process_states_data(spark, input_data, output_data):\n",
    "    # read data\n",
    "    states_table = spark.read.csv('states_addr.csv', header='true')\n",
    "    \n",
    "    # write to parquet files\n",
    "    states_table.write.parquet(output_data + 'states/')   \n",
    "\n",
    "\n",
    "def process_immigration_data(spark, input_data, output_data):\n",
    "    # get filepath to immigration_data file\n",
    "    immigration_data = input_data + 'i94'\n",
    "    \n",
    "    # read immigration data file\n",
    "    df_spark = spark.read.format('com.github.saurfang.sas.spark').load(immigration_data)\n",
    "    \n",
    "    # replace col names with clear names and drop dublikates\n",
    "    columns_to_change = ['cicid as cid', 'i94yr as year', 'i94mon as month', 'i94cit as citizenship', 'i94res as residence', 'i94port as airport', \\\n",
    "                     'arrdate as arrival_date', 'i94mode as arrival_mode', 'i94addr as addr_in_us', 'depdate as departure_date', 'i94bir as age', \\\n",
    "                     'i94visa as visa_code', 'count', 'dtadfile as adfile_date', 'visapost', 'occup', \\\n",
    "                     'entdepa as arrival_code', 'entdepd as departure_code', 'entdepu as update_code', 'matflag', 'biryear', \\\n",
    "                     'dtaddto as admitted_date', 'gender', 'insnum', 'airline', 'admnum', 'fltno as flight_number', \\\n",
    "                     'visatype']\n",
    "    df_spark = df_spark.selectExpr(columns_to_change).dropDuplicates()\n",
    "    \n",
    "    # change type of columns and apply correct formats for dates \n",
    "    df_spark = df_spark\\\n",
    "        .withColumn(\"data_base_sas\", F.to_date(F.lit(\"01/01/1960\"), \"MM/dd/yyyy\"))\\\n",
    "        .withColumn(\"cid\", F.col(\"cid\").cast(\"integer\"))\\\n",
    "        .withColumn(\"year\", F.col(\"year\").cast(\"integer\"))\\\n",
    "        .withColumn(\"month\", F.col(\"month\").cast(\"integer\"))\\\n",
    "        .withColumn(\"citizenship\", F.col(\"citizenship\").cast(\"integer\"))\\\n",
    "        .withColumn(\"residence\", F.col(\"residence\").cast(\"integer\"))\\\n",
    "        .withColumn(\"airport\", F.col(\"airport\").cast(\"string\"))\\\n",
    "        .withColumn(\"arrival_mode\", F.col(\"arrival_mode\").cast(\"integer\"))\\\n",
    "        .withColumn(\"departure_date\", F.expr(\"date_add(data_base_sas, departure_date)\"))\\\n",
    "        .withColumn(\"age\", F.col(\"age\").cast(\"integer\"))\\\n",
    "        .withColumn(\"visa_code\", F.col(\"visa_code\").cast(\"integer\"))\\\n",
    "        .withColumn(\"count\", F.col(\"count\").cast(\"integer\"))\\\n",
    "        .withColumn(\"adfile_date\", F.to_date(F.unix_timestamp(F.col(\"adfile_date\"), \"yyyyMMdd\").cast(\"timestamp\")))\\\n",
    "        .withColumn(\"biryear\", F.col(\"biryear\").cast(\"integer\"))\\\n",
    "        .withColumn(\"admitted_date\", F.to_date(F.unix_timestamp(F.col(\"admitted_date\"), \"MMddyyyy\").cast(\"timestamp\")))\\\n",
    "        .withColumn(\"arrival_date\", F.expr(\"date_add(data_base_sas, arrival_date)\"))\\\n",
    "        .drop('data_base_sas')\n",
    "    \n",
    "    # create personal dimensional table\n",
    "    personal_columns = ['personal_id','cid', 'age', 'adfile_date', 'occup', 'biryear', 'gender', 'insnum', 'admnum', 'citizenship', 'residence', 'addr_in_us', 'visatype', \\\n",
    "                        'flight_number', 'airline', 'visa_code', 'adfile_date']\n",
    "    personal_table = df_spark.withColumn(\"personal_id\", F.monotonically_increasing_id()).select(personal_columns)\n",
    "      \n",
    "    # write personal table to parquet files\n",
    "    personal_table.write.parquet(output_data + 'personal/')\n",
    "    \n",
    "    # create time table dimension\n",
    "    time_table = df_spark.withColumn(\"dayofmonth\", F.dayofmonth(F.col(\"arrival_date\"))) \\\n",
    "                     .withColumn(\"dayofyear\", F.dayofyear(F.col(\"arrival_date\"))) \\\n",
    "                     .withColumn(\"dayofweek\", F.dayofweek(F.col(\"arrival_date\"))) \\\n",
    "                     .select('arrival_date', 'year', 'month', 'dayofmonth', 'dayofyear', 'dayofweek')\n",
    "    \n",
    "    # write table to parquet files\n",
    "    time_table.write.parquet(output_data + 'time/')\n",
    "    \n",
    "    # create airport table\n",
    "    airports_table = df_spark.withColumn(\"airport_id\", F.monotonically_increasing_id()).select('airport_id', 'airport', 'visapost')\n",
    "    \n",
    "    # write table to parquet files\n",
    "    airports_table.write.parquet(output_data + 'airports/')\n",
    "    \n",
    "    # read countries_table\n",
    "    countries_table = spark.read.parquet(output_data + 'countries')\n",
    "    \n",
    "    # create fact table immigration\n",
    "    immigration_table = df_spark.withColumn(\"id\", F.monotonically_increasing_id()).join(countries_table, df_spark.citizenship == countries_table.code, how='left') \\\n",
    "                                .select('id', 'month', 'country')\n",
    "    \n",
    "    # write table to parquet files\n",
    "    immigration_table.write.parquet(output_data + 'immigration/')\n",
    "    \n",
    "def process_demographics_data(spark, input_data, output_data):\n",
    "    # read demographics data\n",
    "    demographics_table = spark.read.csv('demographics.csv', delimiter=';', header='true')\n",
    "    \n",
    "    # missing values in demographic dataframe replace with mode\n",
    "    for column in demographics_table.columns:\n",
    "        demographics_table[column].fillna(demographics_table[column].mode()[0], inplace=True)\n",
    "        \n",
    "    # write to parquet files\n",
    "    demographics_table.write.parquet(output_data + 'demographics/') \n",
    "    \n",
    "def check_tables(spark, output_data):\n",
    "    paths = ['time/', 'immigration/', 'demographics/', 'countries/', 'states/', 'airports/', 'personal/']\n",
    "    for path in paths:\n",
    "        df = spark.read.parquet(output_data + path)\n",
    "        print((df.count(), len(df.columns)))\n",
    "    \n",
    "def main():\n",
    "    spark = create_spark_session()\n",
    "    input_data = 's3n://dend-capstone-bucket/'\n",
    "    output_data = 's3n://dend05-datalake/'\n",
    "    \n",
    "    process_countries_data(spark, input_data, output_data)\n",
    "    process_states_data(spark, input_data, output_data)\n",
    "    process_demographics_data(spark, input_data, output_data)\n",
    "    process_immigration_data(spark, input_data, output_data)           \n",
    "    check_tables(spark)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "    I check the integrity of datasets by counting the number of rows and columns for each table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Run Quality Checks\n",
    "paths = ['time/', 'immigration/', 'demographics/', 'countries/', 'states/', 'airports/', 'personal/']\n",
    "output_data = 's3n://dend05-datalake/'\n",
    "def check_tables():\n",
    "    for path in paths:\n",
    "        df = spark.read.parquet(output_data + path)\n",
    "        print((df.count(), len(df.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "#### Immigration table\n",
    "- *Name:* `immigration`\n",
    "- *Type:* Fact table\n",
    "\n",
    "| Column | Description |\n",
    "| ------ | ----------- |\n",
    "| `id` | The main identification of the table | \n",
    "| `cid` | Unique key within a month |\n",
    "| `month` | Numeric month |\n",
    "| `citizenship` | Immigrant's country of citizenship |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Personal table\n",
    "\n",
    "- *Name:* `personal`\n",
    "- *Type:* Dimension table\n",
    "\n",
    "| Column | Description |\n",
    "| ------ | ----------- |\n",
    "| `id` | The identification of an user |\n",
    "| `cid` | The main identification of an user within a month |\n",
    "| `age` | Immigrant's age in years |\n",
    "| `gender` | The gender is stated with just one character `M` (male), `F` (female), `X` or `U`. Otherwise it can be stated as `NULL` |\n",
    "| `dtadfile` | Date in the format YYYYMMDD |\n",
    "| `occup` | Occupation in US of immigration |\n",
    "| `biryear` | Four-digit year of birth |\n",
    "| `insnum` | Immigration and Naturalization Services number |\n",
    "| `admnum` | Admission number |\n",
    "| `citizenship` | Immigrant's country of citizenship |\n",
    "| `residence` | Immigrant's country of residence outside US |\n",
    "| `addr_in_us` | Address (usually state) of immigrant in US |\n",
    "| `visatype` | Short visa codes like WT, B2, WB, etc.|\n",
    "| `flight_number` | Flight number of immigrant |\n",
    "| `airline` | Airline of entry for immigrant |\n",
    "| `visa_code` | The type of visa the immigrant is coming in on (`1`, `2` or `3`) |\n",
    "| `adfile_date` | Dates in the format YYYYMMDD |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Countries table\n",
    "\n",
    "- *Name:* `countries`\n",
    "- *Type:* Dimension table\n",
    "\n",
    "| Column | Description |\n",
    "| ------ | ----------- |\n",
    "| `code` | Short identification of a country | \n",
    "| `country` | Name of a country |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### States table\n",
    "\n",
    "- *Name:* `States`\n",
    "- *Type:* Dimension table\n",
    "\n",
    "| Column | Description |\n",
    "| ------ | ----------- |\n",
    "| `code` | Short identification of a state | \n",
    "| `state` | Name of a state |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Demographics table\n",
    "\n",
    "- *Name:* `demographics`\n",
    "- *Type:* Dimension table\n",
    "\n",
    "| Column | Description |\n",
    "| ------ | ----------- |\n",
    "| `city` | The main identification of an user |\n",
    "| `state` | Full state name |\n",
    "| `median_age` | The median age per state |\n",
    "| `male_population` | Total Male population per state |\n",
    "| `female_population` | Total Female population per state |\n",
    "| `total_population` | Total population of the state |\n",
    "| `number_of_veterans` | The count of foreign-born per state |\n",
    "| `foreign-born` | The count of people per state |\n",
    "| `average_houshold_size` | The laverage houshold size |\n",
    "| `state_code` | State code |\n",
    "| `race` | Race population ratio per state |\n",
    "| `count` | The count of people per state |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Airports table\n",
    "\n",
    "- *Name:* `airports`\n",
    "- *Type:* Dimension table\n",
    "\n",
    "| Column | Description |\n",
    "| ------ | ----------- |\n",
    "| `id` | The main identification |\n",
    "| `airport` | Port of entry |\n",
    "| `visapost` | Three-letter codes corresponding to where visa was issued |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Project files\n",
    "- `etl.py` - reads data from S3, processes that data using Spark, and writes them back to S3\n",
    "- `dl.cfg` - contains AWS credentials\n",
    "- `Project.ipynb` - the notebook contains the preparatory steps for etl.py with smaller data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "    I used Apache Spark to do all the processing data and create the model. The reason for this is because Spark can scale many data, and the library spark.sql has many tools to transform data. The data that persisted in parquet files can scale to lots of terabytes without any problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "    The data should be updated every day. We can use Apache Airflow to ingest every day (arrival date) because the fact table is partitioned bay arrival date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "    I could also ensure that the data was ready to populate a dashboard by seven AM every day as well. I have to assume I could get the daily data for fact immigration table and, when it is available, I could have an Airflow DAG using an S3Sensor that kicked off upon its arrival and then proceeded to parse the data, land it in its date-partitioned location in S3, in which case it would be ready for Redshift Spectrum to read immediately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "    If the data simultaneously increases by more than a hundred times, and there are many users, you can safely place the data in a column database such as redshift. I don't think there are any problems. Materialized views should also help in this case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
